exp_name: !!str debug
terminated_fig_path: !!str /home/terminated_debug
rl_algorithm: !!str sac

warmup_steer_min: !!float -0.1
warmup_steer_max: !!float 0.1
warmup_throttle_min: !!float 0.4
warmup_throttle_max: !!float 0.8
warmup_aps_min: !!float 0.4
warmup_aps_max: !!float 0.8
warmup_bps_min: !!float -1.
warmup_bps_max: !!float -0.8

replay_buffer_size: !!int 100_000_0


style:
  usage: !!bool false
  size: !!int 0
  level_step_size: !!float 0.5
  
qf1:
  hidden_sizes: 
    - 256
    - 256
  output_size: !!int 1

qf2:
  hidden_sizes:
    - 256
    - 256
  output_size: !!int 1

target_qf1:
  hidden_sizes: 
    - 256
    - 256
  output_size: !!int 1

target_qf2:
  hidden_sizes: 
    - 256
    - 256
  output_size: !!int 1

policy:
  hidden_sizes:
    - 256
    - 256
  layer_norm: !!bool false
  std: !!null # !!float null
  
algorithm:
  batch_size: !!int 256
  max_path_length: !!int 10000 #max path length (1000개보다 많아야 할 것 같음)
  num_epochs: !!int 3000
  num_eval_steps_per_epoch: !!int 5000 #evaluation은 사실상 그냥 logging이나 결과 확인을 위해서 필요하기 때문에 제외해도 됨
  num_expl_steps_per_train_loop: !!int 1000
  num_trains_per_train_loop: !!int 1000
  num_train_loops_per_epoch: !!int 1 #하나의 epoch당 .backward()를 몇번 수행하는지를 지정해 준다고 봐도 됨
  min_num_steps_before_training: !!int 1000
  warmup_actor_step: !!float -1.
  num_step_for_expl_data_collect: !!int 1 


trainer:
  discount: !!float 0.99
  reward_scale: !!float 1.
  policy_lr: !!float 4e-3 #1e-3
  qf_lr: !!float 4e-3
  optimizer_class: !!str Adam
  soft_target_tau: !!float 5e-3
  target_update_period: !!int 1
  use_automatic_entropy_tuning: !!bool true
  alpha_val: !!float 1.
  policy_std_schedule: !!null #!!int null

test:
  test_max_path_length: !!int 1000000
  test_log_path: !!str D:/meow_racer_experiments # /nas/meow_racer_experiments