#!/bin/bash

WORLD_DT=0.1
ACTION_DT=0.01
LOOKAHEAD_TIME=1
MOVE_REWARD_MIN_VEL=8
MOVE_REWARD_MAX_VEL=18
MOVE_REWARD_VALUE=1
MOVE_REWARD_OVERSPEED_PEN=5
MOVE_REWARD_UNDERSPEED_PEN=2

TIME_PEN_USAGE=false

python train_main.py \
    agent.exp_name=SAC_GaussPolicy_0512_Both_Act3_Center_LookTheta_MoveReward_TimePen \
    agent.policy.layer_norm=false \
    agent.algorithm.warmup_actor_step=-1.\
    agent.algorithm.batch_size=256 \
    agent.algorithm.num_epochs=100000 \
    agent.algorithm.min_num_steps_before_training=1000 \
    agent.algorithm.num_expl_steps_per_train_loop=5000 \
    agent.algorithm.num_trains_per_train_loop=5000 \
    agent.algorithm.num_train_loops_per_epoch=1 \
    agent.algorithm.max_path_length=1e+12 \
    agent.algorithm.num_step_for_expl_data_collect=1 \
    agent.replay_buffer_size=1000000 \
    agent.test.test_max_path_length=1e+12 \
    agent.test.test_log_path="D:/meow_racer_experiments" \
    environment.random_seed=77 \
    environment.observation.vx.usage=true \
    environment.observation.vy.usage=true \
    environment.observation.lookahead.usage=true \
    environment.observation.lookahead.lookahead_time=$LOOKAHEAD_TIME \
    environment.observation.lookahead.num_states=20 \
    environment.observation.lookahead.coords=false \
    environment.observation.lookahead.curvature=true \
    environment.observation.lookahead.scale_method=standard \
    environment.observation.lookahead.lookahead_theta=20 \
    environment.observation.lookahead.fixed=theta \
    environment.observation.forward_vector.usage=true \
    environment.observation.forward_vector.rotate_vec=false \
    environment.observation.lidar_sensor.usage=true \
    environment.observation.lidar_sensor.scale_method=minmax \
    environment.observation.lidar_sensor.num_lidar=60 \
    environment.track.track_density=1 \
    environment.vehicle.dt=$ACTION_DT \
    environment.vehicle.world_dt=$WORLD_DT \
    environment.vehicle.brake_on_pos_vel=true \
    environment.vehicle.allow_both_feet=true \
    environment.vehicle.allow_neg_torque=true \
    environment.action.action_dim=3 \
    environment.do_debug_logs=0 \
    environment.track.use_nam_only=false \
    environment.track.nam_ratio=0.1 \
    environment.vehicle.aps_bps_weight=1. \
    penalty.terminate.penalty_value=200 \
    penalty.time_penalty.usage=$TIME_PEN_USAGE \
    penalty.terminate.off_course_condition=off_course_tire \
    penalty.off_course_penalty.usage=true \
    penalty.off_course_penalty.ratio_usage=true \
    penalty.off_course_penalty.penalty_value=2 \
    penalty.off_course_penalty.condition=off_course_instance \
    penalty.E_phi_penalty.usage=false \
    penalty.curvature_vel_penalty.usage=true \
    penalty.curvature_vel_penalty.penalty_value=10 \
    penalty.curvature_vel_penalty.target_theta=20 \
    penalty.curvature_vel_penalty.future_mode=mean \
    penalty.E_c_penalty.usage=true \
    penalty.E_c_penalty.penalty_value=3 \
    penalty.E_c_penalty.normalize_E_c=true \
    reward.progress_reward_curve.usage=false \
    reward.movement_reward.usage=true \
    reward.movement_reward.reward_value=$MOVE_REWARD_VALUE \
    reward.movement_reward.reward_min_vel=$MOVE_REWARD_MIN_VEL \
    reward.movement_reward.reward_max_vel=$MOVE_REWARD_MAX_VEL \
    reward.movement_reward.overspeed_pen_value=$MOVE_REWARD_OVERSPEED_PEN \
    reward.movement_reward.underspeed_pen_value=$MOVE_REWARD_UNDERSPEED_PEN
