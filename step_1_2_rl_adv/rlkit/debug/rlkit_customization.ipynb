{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc94965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "ROOT = r'C:\\Users\\7459985\\Desktop\\2025\\RDDS\\001_code\\meow_racer\\step_1_2_rl_adv'.replace('\\\\', '/')\n",
    "sys.path.append(ROOT)\n",
    "from rlkit.torch.sac.policies.gaussian_policy import TanhGaussianPolicy\n",
    "## rlkit은 policy distribution을 구현하기 위해서 torch.distributions의 확률 분포를 그대로 사용하는 것이 아니라, 따로 Distribution class를 구현해 놓음.\n",
    "## Beta distribution도 구현이 되어 있기 때문에 사용해 봐도 좋을 듯.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f314e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = TanhGaussianPolicy(hidden_sizes=[256,256], obs_dim=33, action_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "721985b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=33, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=256, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for i, fc in enumerate(policy.fcs):\n",
    "    print(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fb1e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal as TorchNormal\n",
    "from rlkit.torch.distributions import TanhNormal\n",
    "from rlkit.torch.sac.policies import MakeDeterministic, TanhGaussianPolicy\n",
    "import numpy as np\n",
    "import torch\n",
    "# dist = TorchNormal(0, 1)\n",
    "# dist = TanhNormal(torch.Tensor([[0, 0]]), torch.Tensor([[1,1]]))\n",
    "policy = TanhGaussianPolicy(hidden_sizes=[256,256],obs_dim=33, action_dim=2)\n",
    "dummy_obs = torch.FloatTensor(np.random.random((10, 33)))\n",
    "dist = policy(dummy_obs)\n",
    "# log_prob = dist.log_prob(torch.FloatTensor(np.array(\n",
    "#     [[[0.2], [-0.4]], [[-0.3], [0.3]]\n",
    "# ]))) #하나의 batch안의 action이 [[action_1], [action_2]]이렇게 2차원이고, 이런게 여러 batch있어야 함.\n",
    "# print(log_prob)\n",
    "# squeezed_log_prob = log_prob.sum(dim=1, keepdim=True)\n",
    "# print(squeezed_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1f415215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(policy.state_dict(), 'test.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91d1750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SACLosses(policy_loss=0, qf1_loss=1, qf2_loss=2, alpha_loss=3)\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "SACLosses = namedtuple(\n",
    "    'SACLosses',\n",
    "    'policy_loss qf1_loss qf2_loss alpha_loss',\n",
    ")\n",
    "loss = SACLosses(policy_loss=0, qf1_loss=1, qf2_loss=2, alpha_loss=3)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "83ee84c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_loss 0\n",
      "qf1_loss 1\n",
      "qf2_loss 2\n",
      "alpha_loss 3\n"
     ]
    }
   ],
   "source": [
    "for key, value in loss._asdict().items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "25ee286a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc0.weight',\n",
       "              tensor([[-0.0322, -0.0443,  0.0458,  ..., -0.0364, -0.0493,  0.0388],\n",
       "                      [ 0.0518, -0.0343,  0.0228,  ..., -0.0376, -0.0567,  0.0300],\n",
       "                      [ 0.0494,  0.0034,  0.0404,  ..., -0.0308, -0.0090, -0.0350],\n",
       "                      ...,\n",
       "                      [-0.0249, -0.0513, -0.0461,  ..., -0.0218,  0.0164,  0.0129],\n",
       "                      [-0.0073, -0.0521, -0.0072,  ..., -0.0478, -0.0159, -0.0220],\n",
       "                      [-0.0272, -0.0363,  0.0571,  ..., -0.0034, -0.0587,  0.0142]])),\n",
       "             ('fc0.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('fc1.weight',\n",
       "              tensor([[ 0.0343,  0.0485,  0.0517,  ..., -0.0218,  0.0362,  0.0339],\n",
       "                      [ 0.0352,  0.0384, -0.0256,  ...,  0.0283, -0.0318, -0.0296],\n",
       "                      [ 0.0129, -0.0008, -0.0611,  ...,  0.0372,  0.0030,  0.0015],\n",
       "                      ...,\n",
       "                      [ 0.0264,  0.0097, -0.0249,  ...,  0.0106, -0.0585, -0.0588],\n",
       "                      [-0.0289,  0.0484, -0.0452,  ...,  0.0104, -0.0560, -0.0061],\n",
       "                      [ 0.0333, -0.0354, -0.0320,  ...,  0.0246,  0.0326,  0.0371]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('last_fc.weight',\n",
       "              tensor([[ 3.4172e-04, -3.2852e-04,  9.2539e-04,  9.1652e-04, -6.1542e-04,\n",
       "                        1.5710e-04,  2.9358e-04,  6.4899e-04,  3.0864e-04, -1.2575e-04,\n",
       "                       -1.4518e-04,  6.7195e-04,  1.2996e-05, -4.6987e-04, -1.9999e-04,\n",
       "                        2.7789e-04,  2.1177e-04,  1.7976e-04,  1.0505e-04, -2.8364e-04,\n",
       "                       -4.4126e-05, -4.2633e-04, -8.2737e-04,  1.2549e-04, -6.5754e-04,\n",
       "                        8.5683e-04,  6.5376e-04,  8.1674e-05,  7.1578e-04, -8.9297e-04,\n",
       "                       -8.7372e-04,  9.3480e-04,  5.1094e-05,  7.3466e-04, -7.3233e-04,\n",
       "                        3.4930e-04,  7.2393e-04, -9.7990e-04,  8.1678e-05, -5.1201e-04,\n",
       "                        4.1001e-04,  2.6619e-04,  4.3657e-06, -9.7499e-04, -6.9212e-04,\n",
       "                        3.5117e-04, -7.2887e-04, -1.2058e-04,  8.9826e-04, -1.0258e-04,\n",
       "                       -3.9263e-05,  4.8914e-04,  4.9750e-04, -4.3831e-04, -8.3547e-04,\n",
       "                       -9.6721e-04, -7.6888e-04, -4.8209e-04,  6.1643e-04, -3.3367e-04,\n",
       "                        6.7566e-04, -8.0400e-04, -3.9865e-05,  8.9292e-04,  4.3685e-04,\n",
       "                       -1.4594e-04,  7.5380e-04,  9.2636e-04, -1.6472e-04, -7.5877e-05,\n",
       "                       -2.7621e-04,  6.0218e-04,  7.7723e-04,  3.0887e-04,  7.7284e-04,\n",
       "                       -4.5052e-04,  9.1235e-04, -2.4741e-04,  1.2311e-04,  1.1323e-04,\n",
       "                        4.2967e-04, -6.6254e-04, -6.0748e-04, -7.6838e-04, -5.8868e-05,\n",
       "                        3.3090e-04, -1.7644e-04,  3.8881e-04, -8.4523e-05, -5.8848e-04,\n",
       "                        3.3322e-04,  2.5867e-04,  4.7879e-04, -8.3480e-04, -5.7200e-04,\n",
       "                        4.1584e-04, -9.8389e-04,  7.3387e-04,  8.7061e-04, -9.5758e-04,\n",
       "                       -9.0889e-04, -8.2919e-04, -3.5451e-04,  8.8767e-04,  7.0097e-04,\n",
       "                       -2.3004e-04,  4.0073e-05, -5.5757e-04, -8.1515e-04,  4.0965e-04,\n",
       "                       -4.2495e-04,  7.8522e-05,  6.1681e-05,  6.6354e-04, -6.9171e-04,\n",
       "                       -6.6043e-04, -3.7229e-04,  5.4201e-04,  7.5595e-05, -2.9419e-04,\n",
       "                       -1.5880e-04,  4.1831e-04, -9.6445e-04,  9.9443e-04,  1.5012e-05,\n",
       "                        5.0997e-04,  9.9895e-04, -6.9038e-04, -8.4057e-04,  3.0069e-04,\n",
       "                       -1.4880e-04, -3.0520e-04,  2.0750e-04, -5.1832e-04,  4.1144e-04,\n",
       "                       -5.1324e-04,  9.0289e-04, -1.6077e-04, -2.4184e-05,  3.3080e-04,\n",
       "                        8.1548e-04,  3.9431e-04, -5.8365e-04, -2.1428e-04, -8.6092e-04,\n",
       "                        4.7886e-05,  5.6936e-04, -2.9834e-05,  8.3288e-05,  3.9428e-04,\n",
       "                       -7.6156e-05, -1.8745e-04, -5.9147e-04,  5.6359e-04, -5.0535e-04,\n",
       "                        8.8316e-04,  4.0103e-05,  6.6289e-04,  1.2630e-05, -2.1310e-04,\n",
       "                       -5.6621e-04, -5.1918e-04,  4.1177e-04,  9.1633e-05,  4.2788e-04,\n",
       "                        2.0190e-04, -6.1721e-04,  2.9884e-04, -8.6418e-04,  3.9993e-05,\n",
       "                        3.5772e-04,  4.1726e-04, -4.4095e-04,  7.7062e-04,  9.7554e-04,\n",
       "                       -3.8633e-04,  5.8258e-04,  3.9187e-04, -8.2223e-04,  7.6676e-04,\n",
       "                       -4.9982e-04, -3.9737e-04,  9.2012e-04, -7.6171e-04, -6.3494e-04,\n",
       "                       -7.3535e-04,  9.4671e-04,  3.3439e-04,  3.2514e-04,  5.6082e-04,\n",
       "                        3.9330e-04,  9.2860e-04, -4.1894e-04, -2.8935e-04, -9.9861e-04,\n",
       "                       -4.0398e-04,  6.6049e-04, -9.6526e-04, -6.8969e-04,  4.9029e-04,\n",
       "                        3.5978e-04, -8.1231e-04,  5.2142e-04,  5.8990e-04, -9.2937e-04,\n",
       "                       -4.8827e-04,  3.1356e-06, -4.2377e-04,  2.3406e-04,  4.0657e-04,\n",
       "                       -3.1181e-04, -7.3462e-04,  8.4465e-04, -8.0781e-04, -3.6889e-04,\n",
       "                       -1.4030e-04, -7.8148e-05,  4.2293e-04, -2.2962e-04, -7.4177e-04,\n",
       "                       -5.6314e-04,  9.4255e-05,  5.6855e-04,  5.8130e-04, -4.0139e-04,\n",
       "                       -7.3706e-04, -4.7717e-04, -1.2722e-04, -5.4850e-04, -1.2211e-04,\n",
       "                       -7.2600e-04, -1.4755e-04, -7.7600e-04,  8.8311e-04,  8.1958e-04,\n",
       "                        2.5443e-04,  5.2580e-04,  1.9800e-04, -8.2610e-04,  7.3594e-04,\n",
       "                       -2.9523e-04, -9.2250e-04, -1.8709e-04,  7.0067e-04,  4.9721e-04,\n",
       "                        2.1300e-04,  2.6748e-04,  2.8739e-04,  2.1185e-04,  3.8694e-04,\n",
       "                       -7.4901e-04,  8.1403e-04, -3.0302e-04,  4.4414e-05, -7.9628e-04,\n",
       "                       -9.4333e-04],\n",
       "                      [-6.9432e-04, -4.6770e-04,  9.5500e-04,  7.7708e-04,  5.7017e-04,\n",
       "                        2.0033e-04,  7.2656e-04, -1.1864e-04,  4.5984e-04,  1.1111e-04,\n",
       "                        2.3935e-04,  4.1909e-04, -2.4291e-04,  4.8285e-04,  4.8733e-05,\n",
       "                        1.2841e-04,  4.1114e-04, -3.6970e-04,  4.5977e-04, -7.9712e-04,\n",
       "                       -1.9632e-04,  3.5960e-04, -1.6897e-04, -8.6849e-05, -4.7401e-05,\n",
       "                       -8.2657e-04,  9.8954e-04, -7.7695e-05, -5.5656e-04,  5.6532e-04,\n",
       "                       -1.5181e-04, -1.1671e-04, -3.7784e-04,  9.7350e-04,  7.3437e-04,\n",
       "                        2.1094e-04, -9.8406e-04, -7.6564e-04, -9.7571e-04,  7.7925e-04,\n",
       "                       -4.7836e-04, -2.6916e-04,  8.2678e-04, -7.3641e-05,  2.7266e-04,\n",
       "                        5.0081e-04,  3.5449e-05, -2.8252e-04, -7.0380e-04,  1.2531e-04,\n",
       "                       -8.6362e-04,  1.7282e-04, -4.4697e-05,  4.6171e-04,  5.6993e-04,\n",
       "                        7.8732e-04,  8.5283e-04, -6.5718e-04,  7.7144e-04, -8.0738e-05,\n",
       "                       -4.2289e-04,  8.2971e-04, -6.5955e-04,  6.2834e-04,  9.1582e-04,\n",
       "                        1.6118e-04, -9.4700e-04, -1.0945e-05, -4.6567e-04,  6.9166e-04,\n",
       "                       -6.3978e-04,  3.1921e-04, -2.2719e-04, -1.8988e-04, -9.7520e-04,\n",
       "                       -3.8253e-04, -4.8772e-04,  1.7831e-04,  3.7154e-04, -1.5339e-04,\n",
       "                        6.8296e-04, -1.0195e-04,  6.9082e-04,  5.8386e-04, -6.8037e-05,\n",
       "                        1.8025e-04, -9.3661e-04,  5.6292e-04, -2.1014e-04,  2.9728e-04,\n",
       "                       -5.8749e-04,  8.5876e-04,  3.8883e-04, -6.5614e-04,  1.6477e-04,\n",
       "                       -8.0128e-04,  7.8727e-05, -7.0841e-04, -9.8783e-04, -7.9834e-04,\n",
       "                       -7.0270e-04, -7.9833e-04,  3.5327e-04,  2.0075e-04, -5.0514e-04,\n",
       "                        7.7422e-04,  1.8828e-04,  7.5528e-05, -8.5343e-04,  7.2066e-04,\n",
       "                       -7.7182e-04,  3.7566e-04, -1.5704e-04,  3.8379e-04, -6.9635e-04,\n",
       "                       -8.6164e-04,  2.1997e-04, -5.7587e-04,  8.3273e-04, -1.1295e-05,\n",
       "                       -1.5683e-04,  2.0943e-04, -8.9459e-04,  4.6671e-04,  6.7558e-04,\n",
       "                       -1.2080e-04,  1.0199e-04, -1.6037e-04, -2.0605e-04, -6.4689e-04,\n",
       "                       -1.5431e-04,  3.3137e-04,  6.7920e-04,  9.9551e-04,  2.4242e-04,\n",
       "                        2.8915e-04, -3.7047e-04,  8.5676e-04,  4.6133e-04,  7.9925e-04,\n",
       "                       -6.6495e-04, -8.2373e-04,  9.6672e-04, -5.6524e-04,  1.7972e-04,\n",
       "                        9.2328e-04,  1.7063e-04,  1.0643e-04, -4.1180e-04,  8.1844e-04,\n",
       "                        4.1274e-04, -5.8714e-04, -4.2825e-04, -9.1252e-04,  2.2396e-04,\n",
       "                        2.0446e-04,  4.9634e-04,  4.2634e-04,  3.6752e-04, -8.9708e-04,\n",
       "                        6.2244e-05, -7.2805e-04, -2.2483e-04,  6.0118e-04,  2.5485e-04,\n",
       "                        8.2435e-04,  8.8200e-04, -6.8923e-04,  9.9804e-04, -7.1121e-04,\n",
       "                       -5.2279e-05, -1.1162e-04, -5.7500e-04,  2.2741e-04, -4.5292e-04,\n",
       "                        5.8055e-04, -6.4758e-04, -6.0017e-04,  9.9028e-04, -2.4923e-04,\n",
       "                        1.2957e-04, -4.7203e-04, -2.4274e-04, -4.2101e-04,  7.0996e-04,\n",
       "                       -8.8493e-04, -8.3632e-04, -7.8251e-04,  5.4892e-04, -9.8751e-04,\n",
       "                        6.1929e-05,  6.6736e-04, -9.9459e-04,  2.8267e-04, -4.0549e-04,\n",
       "                        3.9165e-04,  7.6815e-05,  7.9461e-04, -9.7890e-04,  3.2920e-04,\n",
       "                        1.1677e-04,  7.9544e-04, -1.8650e-04,  1.7532e-04, -6.3253e-05,\n",
       "                        3.7295e-04, -2.2036e-04,  4.7454e-04, -3.1676e-04, -3.0775e-04,\n",
       "                        9.3964e-04,  7.7134e-04,  4.9906e-04,  3.5117e-04, -8.7932e-04,\n",
       "                       -8.8896e-05,  1.5327e-04, -2.2499e-04,  9.4871e-04, -9.8249e-04,\n",
       "                       -2.1857e-04,  6.1201e-04, -3.9312e-04,  9.4682e-04,  7.6784e-04,\n",
       "                        7.0507e-04, -4.1491e-04,  3.7993e-04,  5.0026e-04, -1.8545e-04,\n",
       "                       -3.5110e-04,  1.9726e-04, -6.0049e-04, -2.6596e-04,  4.7244e-04,\n",
       "                        8.5290e-04, -3.8194e-04, -2.6126e-05, -9.1592e-04,  9.5646e-04,\n",
       "                       -5.2693e-04,  1.2375e-04,  7.0342e-04,  8.2074e-04,  3.5653e-04,\n",
       "                       -1.9977e-05,  6.2272e-04, -2.2191e-04, -3.9496e-04,  4.3315e-04,\n",
       "                       -4.6938e-04,  7.8017e-04,  3.3995e-04, -1.4128e-04,  6.8019e-04,\n",
       "                        4.9682e-04]])),\n",
       "             ('last_fc.bias', tensor([0., 0.])),\n",
       "             ('last_fc_log_std.weight',\n",
       "              tensor([[ 3.3780e-04, -2.4311e-04, -4.0959e-04, -1.6154e-04,  2.2738e-04,\n",
       "                        5.0636e-04,  1.3834e-04,  5.3764e-04,  5.9280e-04, -7.4322e-04,\n",
       "                        4.0965e-04, -3.9207e-04, -6.2741e-04,  6.6719e-04, -3.4315e-04,\n",
       "                        4.1340e-04,  1.7465e-04,  1.2234e-04,  9.7595e-04,  3.9764e-04,\n",
       "                        7.0374e-04,  3.0228e-04,  1.1123e-04, -9.5791e-04, -5.2704e-04,\n",
       "                        9.7748e-04, -3.7519e-04, -7.5711e-04,  6.9081e-04, -5.0840e-04,\n",
       "                        4.2668e-04, -5.6232e-04,  6.0210e-04,  3.2907e-04, -8.5587e-04,\n",
       "                        6.4594e-04, -3.1177e-04,  5.9610e-04, -9.7841e-04,  3.0349e-04,\n",
       "                        5.5910e-04,  5.9983e-04, -8.4644e-04,  4.3804e-04, -7.2698e-04,\n",
       "                        7.2538e-04, -6.1373e-04, -4.6812e-04, -5.7621e-04, -3.0772e-05,\n",
       "                       -7.3159e-04,  9.1909e-04,  2.8746e-04, -2.8527e-04,  4.6045e-04,\n",
       "                        4.6318e-04,  3.4509e-04,  7.4033e-04,  5.6805e-04, -9.9262e-04,\n",
       "                        8.1700e-04, -3.2147e-04, -8.0961e-04, -5.1609e-04,  4.4857e-04,\n",
       "                        9.2037e-04,  2.1927e-04,  4.5845e-04, -5.5266e-04, -4.2990e-04,\n",
       "                        7.0571e-04,  2.5129e-04,  6.7436e-04,  2.9676e-04, -9.4982e-04,\n",
       "                        2.7480e-04,  8.0958e-04, -1.7129e-04, -7.8845e-04,  7.0080e-04,\n",
       "                        1.5456e-04,  5.1152e-04,  5.2586e-04,  2.1734e-04,  7.0517e-04,\n",
       "                        3.3347e-04, -1.6778e-04, -7.6389e-04,  5.9153e-04,  5.9049e-04,\n",
       "                       -2.4320e-04,  6.8863e-04,  2.9319e-04, -7.8114e-04, -9.0645e-04,\n",
       "                        4.4099e-04, -8.7752e-04, -5.6401e-04, -5.6907e-04, -1.8006e-04,\n",
       "                        3.3706e-04,  4.6847e-04,  5.3929e-04, -6.5361e-05,  8.7116e-04,\n",
       "                       -9.6332e-04,  4.4755e-04,  4.5579e-05,  6.6921e-04,  4.0230e-04,\n",
       "                       -1.6653e-04, -3.3717e-04, -3.2665e-04, -5.5516e-04,  8.4733e-04,\n",
       "                       -6.3784e-04,  4.6099e-04, -1.2008e-04,  9.5117e-04,  7.9730e-04,\n",
       "                        2.2795e-04,  1.1697e-04, -7.5675e-04, -1.6592e-04, -7.8989e-04,\n",
       "                       -9.9372e-04,  4.8678e-04,  9.0499e-04, -9.9545e-04,  1.1406e-04,\n",
       "                       -6.9188e-04,  1.7749e-04,  5.8760e-04, -5.8341e-05, -6.5582e-04,\n",
       "                        6.9333e-04, -2.9158e-04,  3.9840e-04, -9.1791e-04,  2.4894e-04,\n",
       "                       -4.0576e-04,  1.7646e-04, -4.7335e-04, -9.8049e-04,  9.7038e-04,\n",
       "                        3.2521e-04,  3.6166e-04, -6.1346e-04,  4.1327e-04,  3.3644e-04,\n",
       "                        9.6251e-04, -1.9935e-04, -5.4801e-04, -5.3713e-04,  3.3516e-04,\n",
       "                        6.0234e-04, -4.4687e-04, -5.6283e-05,  5.1516e-04,  4.4220e-04,\n",
       "                        1.9412e-04,  3.1963e-04, -2.7442e-04,  5.5570e-04,  8.5153e-04,\n",
       "                        6.0598e-04,  7.5007e-04, -6.4468e-04,  2.5983e-04,  9.6822e-04,\n",
       "                       -1.8639e-04,  8.2210e-04, -6.8886e-04, -3.0490e-04, -2.4934e-04,\n",
       "                       -4.8514e-04,  1.4585e-04, -3.5280e-04,  1.5999e-04,  9.6990e-04,\n",
       "                        8.0102e-05,  6.9836e-04, -5.3400e-04, -1.0780e-04,  7.7905e-04,\n",
       "                        6.1641e-04, -9.8655e-04, -3.8342e-04, -1.9042e-04, -2.2221e-04,\n",
       "                       -9.9817e-04, -1.1385e-04, -1.0350e-04,  9.4853e-04, -1.0025e-04,\n",
       "                       -8.5264e-04, -6.9297e-04, -7.1906e-05, -9.6454e-04, -9.8448e-04,\n",
       "                        9.2236e-04,  7.1246e-04,  3.2600e-04, -6.8345e-04, -7.6265e-04,\n",
       "                       -3.4909e-05, -7.3693e-04, -6.5734e-04,  9.6014e-04,  6.9209e-04,\n",
       "                        5.8426e-04,  8.6845e-04,  6.8722e-04,  9.3575e-04,  2.1674e-04,\n",
       "                        2.5067e-04,  3.1112e-04, -4.4854e-04, -8.3044e-04, -7.8611e-04,\n",
       "                       -7.8274e-04,  7.0420e-04, -8.2855e-04,  9.4863e-04, -3.9166e-04,\n",
       "                       -2.8644e-04,  8.8726e-04,  6.9433e-04,  4.8442e-04, -1.4884e-04,\n",
       "                        5.5916e-04, -6.9813e-04, -6.5877e-04, -8.4955e-04,  9.6915e-05,\n",
       "                        5.7960e-04, -5.4034e-04, -5.6101e-04, -3.0991e-04,  6.0720e-04,\n",
       "                        6.1066e-04, -6.4615e-05,  9.5852e-04, -5.3519e-04, -5.2314e-04,\n",
       "                       -5.6676e-04,  4.0145e-04,  8.5935e-04, -9.1452e-04, -8.4995e-05,\n",
       "                        1.4344e-04, -9.2036e-04,  2.5850e-04, -8.8124e-04, -3.8021e-05,\n",
       "                       -3.8463e-04],\n",
       "                      [-3.5136e-04, -8.7778e-04, -6.5687e-06,  7.3061e-04,  7.7955e-04,\n",
       "                        5.4225e-04, -5.0859e-04, -5.8216e-04, -4.2661e-04, -1.9025e-04,\n",
       "                       -9.4629e-04, -2.1522e-04,  5.8149e-04, -8.7873e-04,  8.4213e-04,\n",
       "                       -2.8984e-04,  7.5852e-04,  7.8159e-05,  3.9625e-04, -7.2406e-04,\n",
       "                        5.2994e-04,  3.4677e-04, -9.1817e-04,  8.4878e-04, -6.4029e-04,\n",
       "                       -3.4181e-04, -4.0795e-04,  4.1835e-04,  2.3454e-04, -5.8603e-04,\n",
       "                        2.2362e-06,  7.6451e-04,  3.0200e-04,  7.5529e-04,  2.0490e-05,\n",
       "                        2.0398e-04,  8.0331e-04, -5.5416e-04,  7.3764e-04, -1.9675e-04,\n",
       "                        9.0993e-04,  8.5387e-04,  4.0993e-04,  9.5828e-05, -6.0649e-04,\n",
       "                        5.0553e-04, -5.3094e-04, -3.0629e-04,  8.9399e-04, -3.9916e-04,\n",
       "                        6.8991e-04,  1.1610e-04,  2.5103e-04, -6.8775e-04, -1.3218e-05,\n",
       "                       -9.5683e-04,  3.0748e-04,  5.8209e-04,  6.7155e-05,  6.6518e-04,\n",
       "                       -2.1981e-04,  1.7174e-04, -4.1677e-04,  5.4789e-04, -1.7486e-04,\n",
       "                        6.8029e-05, -9.9588e-04, -6.5729e-04, -3.6025e-04,  1.4546e-04,\n",
       "                       -2.3747e-04,  9.8571e-04, -4.8483e-04, -2.5858e-04, -7.9941e-04,\n",
       "                        4.2074e-04, -4.5919e-04, -1.3541e-04, -4.2311e-04,  2.2159e-04,\n",
       "                       -8.0857e-04,  6.5139e-04,  5.5174e-05,  8.0942e-04,  7.4346e-04,\n",
       "                       -3.6315e-04,  5.6005e-04,  2.2449e-04,  3.7169e-04,  9.1959e-04,\n",
       "                       -8.1727e-04,  4.5662e-04, -9.3934e-04, -3.1456e-04,  1.4410e-04,\n",
       "                        6.3858e-04,  5.5104e-05,  8.4288e-04,  3.8805e-04,  1.3563e-04,\n",
       "                        3.7240e-04, -8.2633e-04, -1.7941e-04, -8.6550e-04,  9.9810e-04,\n",
       "                       -3.1338e-05,  7.6929e-04,  2.7521e-04, -6.8990e-04,  5.8692e-04,\n",
       "                        7.6212e-04,  7.5191e-04, -4.9190e-05,  4.2512e-04,  1.1291e-04,\n",
       "                        7.0914e-04,  2.6781e-04,  3.2497e-04,  3.7838e-04,  2.3525e-04,\n",
       "                       -5.3169e-04,  7.1795e-04, -6.9808e-04,  2.4078e-04, -3.0602e-04,\n",
       "                       -4.9447e-04,  8.9399e-04,  5.3096e-04,  6.5023e-04,  1.3013e-04,\n",
       "                       -3.0556e-05, -1.2263e-04,  1.9254e-04, -3.9169e-04,  6.1526e-04,\n",
       "                        8.2659e-04, -8.2803e-04,  1.8603e-04, -1.2720e-04, -7.0886e-04,\n",
       "                        3.5829e-04, -5.0740e-04,  5.3282e-04, -9.1033e-04,  3.4850e-04,\n",
       "                        2.5503e-05, -8.6630e-04, -6.2037e-04, -9.7212e-04,  9.7839e-04,\n",
       "                        2.2408e-05,  3.8391e-04,  2.8807e-04,  5.5951e-04, -4.3425e-04,\n",
       "                        9.8761e-04,  2.3633e-04, -3.9638e-05, -3.3107e-04, -1.0114e-04,\n",
       "                        7.8615e-04,  3.2702e-04, -2.7847e-04,  7.3433e-04, -5.3894e-04,\n",
       "                        9.8614e-04,  8.9688e-04, -2.5267e-04, -1.7306e-04, -8.0201e-04,\n",
       "                        9.7492e-04, -5.4628e-04, -6.5610e-04, -3.0432e-04, -1.6541e-04,\n",
       "                        7.2023e-04,  5.7650e-04,  5.5302e-04, -5.9554e-04,  6.7740e-05,\n",
       "                       -3.3334e-04,  4.4531e-06,  7.2937e-04,  9.1991e-04,  4.1552e-04,\n",
       "                        3.7517e-04,  7.4731e-04,  1.4502e-04,  6.2505e-04, -4.8064e-04,\n",
       "                        6.5077e-04, -8.1944e-04, -7.7122e-04,  6.1049e-04, -3.0860e-04,\n",
       "                       -7.0896e-04, -1.6357e-04,  8.2021e-04, -7.7812e-04,  3.2397e-04,\n",
       "                       -7.0915e-04,  6.0945e-04, -2.1640e-04,  9.2234e-04,  3.1747e-04,\n",
       "                        6.9235e-04, -1.4646e-04,  7.0909e-04,  9.6429e-04, -1.7162e-05,\n",
       "                       -8.0222e-04,  3.5363e-04, -5.0330e-04, -8.4410e-04,  6.1465e-04,\n",
       "                       -7.9433e-04, -9.8867e-04,  6.9676e-04, -3.5340e-04, -8.5580e-04,\n",
       "                       -6.7797e-04,  9.5199e-05, -1.4199e-04,  8.2527e-04,  1.3802e-04,\n",
       "                       -5.3367e-05, -4.1089e-04, -6.5729e-04,  4.7393e-04,  3.7758e-04,\n",
       "                       -3.6409e-04,  8.7892e-04, -3.5278e-04, -8.4984e-04,  6.3504e-04,\n",
       "                       -3.0278e-05,  4.0287e-04,  2.6272e-04,  6.7680e-04, -6.3621e-04,\n",
       "                       -7.3024e-04,  2.1963e-04,  5.9103e-04,  7.0869e-04,  8.4343e-04,\n",
       "                        8.3032e-04, -5.0598e-04, -3.7924e-05,  7.7132e-05, -6.5044e-04,\n",
       "                       -3.3874e-04, -7.0128e-04,  4.6098e-04,  5.3501e-04,  3.7618e-04,\n",
       "                       -5.0978e-04]])),\n",
       "             ('last_fc_log_std.bias', tensor([-0.0002, -0.0002]))])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0947c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obs_actions, log_pi = dist.rsample_and_logprob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1d6c86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7626, -0.2662],\n",
       "        [-0.5262,  0.4784],\n",
       "        [ 0.7537, -0.9628],\n",
       "        [-0.8191,  0.4484],\n",
       "        [-0.4677, -0.2465],\n",
       "        [ 0.6115, -0.2867],\n",
       "        [ 0.8072, -0.6073],\n",
       "        [ 0.8721,  0.7336],\n",
       "        [ 0.9635, -0.5448],\n",
       "        [ 0.4816, -0.5443]], grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_obs_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9a7b9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4332, -1.5607, -0.8287, -1.2842, -1.6885, -1.5811, -1.1985, -0.9745,\n",
       "        -1.0224, -1.5473], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0c1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0005,  0.0002]], grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_policy = MakeDeterministic(policy) #delta distribution\n",
    "single_obs = torch.Tensor(np.random.random((1, 33)))\n",
    "delta_dist = eval_policy(single_obs) #현재 observation을 기반으로 mean, std가 neural net에 의해 계산되면 이로 distribution을 build해서 return\n",
    "delta_dist.mean #결국에 정규 분포를 따른다고 한다면, distribution의 평균이 deterministic한 policy라고 가정할 때 Q-function의 최대치를 기록하게 하는 action이라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d9e12e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlkit.samplers.data_collector.path_collector import MdpPathCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b6f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_single_rollout(env, agent, obs, preprocess_obs_for_policy_fn=None):\n",
    "    '''[TODO]\n",
    "    1. 어차피 environment normalization wrapper을 사용하는데, 혹시 observation_normalization_fn이 있어야 하는지 확인 필요\n",
    "    '''\n",
    "    rollout_dict = {}\n",
    "    \n",
    "    if preprocess_obs_for_policy_fn is not None:\n",
    "        preprocess_obs_for_policy_fn = lambda x: x\n",
    "        \n",
    "    o_for_agent = preprocess_obs_for_policy_fn(obs)\n",
    "    # get distribution from observation -> samples from the distribution\n",
    "    # when evaluation mode, the distribution is Delta distribution (deterministic)\n",
    "    action, agent_info = agent.get_action(o_for_agent, **get_action_kwargs) # action, empty dict\n",
    "    \n",
    "    \n",
    "    env_step_ret = env.step(copy.deepcopy(action))\n",
    "    if len(env_step_ret) == 5:\n",
    "        next_o, r, terminated, truncated, env_info = env_step_ret\n",
    "        done = terminated | truncated\n",
    "    elif len(env_step_ret) == 4:\n",
    "        next_o, r, done, env_info = env_step_ret\n",
    "    \n",
    "    \n",
    "    rollout_dict['observation'] = next_o\n",
    "    rollout_dict['action'] = action\n",
    "    rollout_dict['terminal'] = done\n",
    "    \n",
    "    return rollout_dict\n",
    "    \n",
    "    \n",
    "def evaluate(eval_policy, \n",
    "             eval_env,\n",
    "             max_path_length,\n",
    "             ):\n",
    "    '''evaluation code referenced from the MdpPathCollector in samplers/data_collector/path_collector.py,\n",
    "    and rollout function from samplers/rollout_functions.py\n",
    "    (하나의 track만을 사용해서 )'''\n",
    "\n",
    "    path_length = 0\n",
    "    eval_policy.reset()\n",
    "    o, env_info_dict = eval_env.reset()\n",
    "    \n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    terminal = []\n",
    "    \n",
    "    while path_length < max_path_length:\n",
    "        single_rollout_dict = _get_single_rollout(env=eval_env, agent=eval_policy)\n",
    "        \n",
    "        observations.append(rollout_dict['observation'])\n",
    "        actions.append(rollout_dict['action'])\n",
    "        is_done = rollout_dict['terminal']\n",
    "        terminal.append(is_done)\n",
    "        if is_done:\n",
    "            break\n",
    "\n",
    "    actions = np.array(actions)\n",
    "    if len(actions.shape) == 1:\n",
    "        actions = np.expand_dims(actions, 1)\n",
    "    observations = np.array(observations)\n",
    "    rewards = np.array(rewards)\n",
    "    if len(rewards.shape) == 1:\n",
    "        rewards = rewards = rewards.reshape(-1, 1) #보통 상수이기 때문에 reshape 사용\n",
    "    \n",
    "    return {\n",
    "        'observations': observations,\n",
    "        'rewards': rewards,\n",
    "        'actions': actions,\n",
    "        'terminal': terminal\n",
    "    }\n",
    "    \n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
